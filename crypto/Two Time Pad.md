# ChronosCTF Write-up

`[Team : 0bscuri7y]`
`[Author : de8U9`

---

## Challenge Information
    
-   **Challenge:** `[Two Time Pad]`
    
-   **Category:** `[Crypto]`

---

## Challenge Description

```
Two time pad????
```

---

## Files Provided

```
[source.py]
[out.txt]
```

---

## Solution

[source.py]
```python
from Crypto.Cipher import AES
from Crypto.Util.Padding import pad
from os import urandom

DATA = b"Everything is coincidence. But even coincidence is part of fate's design. Anyways, here is the flag - saic{--REDACTED--}".hex()

B_SIZE = 16
B_CNT = 48

encoding: dict[str, bytes] = {}
for c in "0123456789abcdef":
    encoding[c] = b"".join([pad(urandom(1), B_SIZE) for _ in range(B_CNT)])

def block_xor(pt: bytes, x: bytes):
    return bytes([pt[i] ^ x[i % len(x)] for i in range(len(pt))])

def encrypt(pt: str):
    ct = b""
    for c in pt:
        ct += block_xor(encoding[c], urandom(B_SIZE))
    return AES.new(key=urandom(16), mode=AES.MODE_ECB).encrypt(ct)

with open("out.txt", "wb") as f:
    _ = f.write(encrypt(DATA))

```

> After analysing it , we can make a few observations it is concatenating encoding of each hex string character of DATA, where each encoded value is 48*16 = 768 bytes long since the encryption scheme is ECB , the same input of 16 byte block is going to produce same output block

> Which means that to decrypt the DATA we need to build a mapping for each encrypted 768 byte block to its original character

> Now, since its using the same random number to xor all the 48 blocks, the result will contain duplicate blocks since there is a very good chance that the 48 blocks encoding generated by taking a single random byte has duplicates

> We can use this to build a identifier for a encrypted 768 byte block of a particular character by finding the index of duplicate values and then use this mapping to decrypt rest of the DATA

### Full Code

```python
known = b"Everything is coincidence. But even coincidence is part of fate's design. Anyways, here is the flag - saic{".hex()
file = open("out.txt","rb").read()
def find_duplicate_indices(lst):
    from collections import defaultdict
    
    index_map = defaultdict(list)
    
    # Store indices of each element
    for index, value in enumerate(lst):
        index_map[value].append(index)
    
    # Generate unique index pairs
    result = []
    for indices in index_map.values():
        if len(indices) > 1:
            result.extend((indices[i], indices[j]) for i in range(len(indices)) for j in range(i + 1, len(indices)))
    return result

hashmap = {}
for i in range(len(known)):
    encrypted=file[i*16*48:(i+1)*16*48]
    chunks = []
    for j in range(0,len(encrypted),16):
        chunks.append(encrypted[j:j+16])
    assert len(chunks) == 48
    dups = find_duplicate_indices(chunks)
    hashmap[known[i]] = dups
    
string = ""
for i in range(0,len(file),16*48):
    data = file[i:i+16*48]
    passed=False
    chunks = []
    for i in range(0,len(data),16):
        chunks.append(data[i:i+16])
    dups = find_duplicate_indices(chunks)

    for c in hashmap:
        if(hashmap[c] == dups):
            string += c
            passed=True
            break
    if(not passed):
        string += "?"
print(known)
print(string)
print(bytes.fromhex(string).decode())

```

---

## Flag

```css
saic{Wh0s3_Ey35_aR3_7h0se_3y35?--fun^10xint^40=Ir2}
```
